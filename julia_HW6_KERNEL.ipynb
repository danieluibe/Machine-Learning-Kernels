{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Downloading and installing Convex libraries by Madeleine Udel\n",
    "# Pkg.update();\n",
    "try\n",
    "    using PyPlot;\n",
    "catch\n",
    "    Pkg.add(\"PyPlot\");\n",
    "    Pkg.build(\"PyPlot\");\n",
    "end\n",
    "try\n",
    "    using MAT;\n",
    "catch\n",
    "    Pkg.add(\"MAT\");\n",
    "    Pkg.build(\"MAT\");\n",
    "end\n",
    "try\n",
    "    using SCS;\n",
    "catch\n",
    "    Pkg.add(\"SCS\");\n",
    "end\n",
    "try\n",
    "    using Convex;\n",
    "catch\n",
    "    Pkg.add(\"Convex\");\n",
    "    Pkg.build(\"Convex\");\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--announcements-->\n",
    "<blockquote>\n",
    "    <center>\n",
    "    <a href=\"http://allyouneedismyth.com/wp-content/uploads/2013/10/yin-yang-copy.png\"><img src=\"yinyang.png\" width=\"400px\" /></a>\n",
    "    </center>\n",
    "      <p><cite><center>\"Just as we have two eyes and two feet,<br>\n",
    "      duality is part of life.\"<br>\n",
    "<b>--Carlos Santana</b><br>\n",
    "      </center></cite></p>\n",
    "</blockquote>\n",
    "\n",
    "\n",
    "<h3>Introduction</h3>\n",
    "In this project, you will implement a linear support vector machine and one operating in kernel space. For this you will need to formulate the primal and dual optimization problems as quadratic programs. You will be using <code>Convex.jl</code>. Before we get started please read through the <a href=\"http://convexjl.readthedocs.io/en/latest/quick_tutorial.html\">tutorial</a> of Convex.jl and its quadratic programming solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Linear classification</h4>\n",
    "\n",
    "<p> The first assignment is to implement a linear support vector machine. Before we get started we can generat some data to see if everything is working:  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function genrandomdata(;N=100,b=0.)\n",
    "    xTr=randn(N,2); #generate random data and linearly separagle labels\n",
    "    w0=rand(2,1); # defining random hyperplane\n",
    "    yTr=sign(xTr*w0+b); # assigning labels +1, -1 labels depending on what side of the plane they lie on\n",
    "    return(xTr,yTr)\n",
    "end;\n",
    "\n",
    "using PyPlot\n",
    "include(\"visclassifier.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Remember the SVM primal formulation\n",
    "$$\\begin{aligned}\n",
    "             &\\min_{\\mathbf{w},b,\\xi} \\|\\mathbf{w}\\|^2_2+C \\sum_{i=1}^n \\xi_i\\\\\n",
    "       & \\text{such that }  \\ \\forall i:\\\\\n",
    "             & y_i(\\mathbf{w}^\\top \\mathbf{x}_i+b)\\geq 1-\\xi_i\\\\\n",
    "             & \\xi_i\\geq 0.\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "You will need to implement  the function <code>SVMprimal</code>, which  takes is important training data <code>xTr</code> ($n\\times d$) and labels <code>yTr</code> ($n\\times 1$) with <code>yTr[i]</code>$\\in \\{-1,1\\}$. Currently the solves a placeholder  problem. You need to update the objective, the constraints and introduce new variables to output the correct hyperplane and bias. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Convex;\n",
    "using SCS;\n",
    "#<GRADED>\n",
    "function primalSVM(xTr,yTr;C=1);\n",
    "    \"\"\"\n",
    "    function (classifier,w,b) = primalSVM(xTr,yTr;C=1)\n",
    "    constructs the SVM primal formulation and uses a built-in \n",
    "    convex solver to find the optimal solution. \n",
    "    \n",
    "    Input:\n",
    "        xTr   | training data (nxd)\n",
    "        yTr   | training labels (nx1)\n",
    "        C     | the SVM regularization parameter\n",
    "    \n",
    "    Output:\n",
    "        fun   | usage: predictions=fun(xTe);\n",
    "        wout  | the weight vector calculated by the solver\n",
    "        bout  | the bias term calculated by the solver\n",
    "    \"\"\"\n",
    "    y=yTr[:];\n",
    "    N,d=size(xTr);\n",
    "    wout=rand(d);\n",
    "    bout=0;\n",
    "    # dummy code\n",
    "    w=Variable(d);\n",
    "    b=Variable(1);\n",
    "    objective=sumsquares(w);\n",
    "    constraints=[w>=0];\n",
    "    problem = minimize(objective,constraints);\n",
    "    solve!(problem);    \n",
    "    wout=w.value\n",
    "    bout=b.value\n",
    "    # End of dummy code\n",
    "    \n",
    "    # TODO 1\n",
    "    \n",
    "    fun=xTe->xTe*wout+bout;\n",
    "    return(fun,wout,bout)\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test your SVM primal solver with the following randomly generated data set. We label it in a way that it is guaranteed to be linearly separable. If your code works correctly the hyper-plane should separate all the $x$'s into the red half and all the $o$'s into the blue half. With sufficiently large values of $C$ (e.g. $C>10$) you should obtain $0\\%$ training error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This may take a while to run the first time you call it\n",
    "xTr,yTr=genrandomdata();\n",
    "fun,w,b=primalSVM(xTr,yTr,C=10); # train svm\n",
    "visclassifier(fun,xTr,yTr,w=w,b=b); # visualize decision boundary\n",
    "err=mean(sign(fun(xTr)).!=yTr); # compute error \n",
    "@printf(\"Training error:%2.1f%%\\n\",err*100); # print it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Spiral data set</h4>\n",
    "\n",
    "<p>The linear classifier works great in simple linear cases. But what if the data is more complicated? We provide you with a \"spiral\" data set. You can load it and visualize it with the following two code snippets:\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load spiral data\n",
    "using MAT;\n",
    "\n",
    "function spiraldata(N=300)\n",
    "    r=linspace(1,2*pi,N);\n",
    "    xTr1=[sin(2.*r[:]).*r cos(2.*r[:]).*r];\n",
    "    xTr2=[sin(2.*r[:]+pi).*r cos(2.*r[:]+pi).*r];\n",
    "    xTr=vcat(xTr1,xTr2);\n",
    "    yTr=vcat(ones(N,1),-ones(N,1));\n",
    "    xTr=xTr+randn(size(xTr)).*0.2;\n",
    "    \n",
    "    xTe=xTr[1:2:end,:];\n",
    "    yTe=yTr[1:2:end];\n",
    "    xTr=xTr[2:2:end,:];\n",
    "    yTr=yTr[2:2:end];\n",
    "    return(xTr,yTr,xTe,yTe)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "figure();\n",
    "clf();\n",
    "xTr,yTr,xTe,yTe=spiraldata();\n",
    "# visualize data\n",
    "\n",
    "i1=find(yTr.==1);\n",
    "i2=find(yTr.==-1);\n",
    "plot(xTr[i1,1],xTr[i1,2],\"b.\");\n",
    "hold(true);\n",
    "plot(xTr[i2,1],xTr[i2,2],\"r.\");\n",
    "\n",
    "legend([\"+1\",\"-1\"]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If you apply your previously functioning linear classifier on this data set you will see that you get terrible results. Even your training error will be too high.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply linear classifier on the spiral data set\n",
    "using PyPlot\n",
    "include(\"visclassifier.jl\");\n",
    "fun,w,b=primalSVM(xTr,yTr,C=10);\n",
    "visclassifier(fun,xTr,yTr,w=[],b=0);\n",
    "err=mean(sign(fun(xTr)).!=yTr);\n",
    "@printf(\"Training error:%2.1f%%\\n\",err*100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementing a kernelized SVM</h3>\n",
    "\n",
    "<p> For something as complex as the spiral data set, you need a more complex classifier. \n",
    "First implement the kernel function\n",
    "<pre>\tcomputeK(kerneltype,X,Z,kpar)</pre>\n",
    "It takes as input a kernel type (kerneltype) and two data sets $\\mathbf{X}$ in $\\mathcal{R}^{n\\times d}$ and $\\mathbf{Z}$ in $\\mathcal{R}^{m\\times d}$ and outputs a kernel matrix $\\mathbf{K}\\in{\\mathcal{R}^{n\\times m}}$. The last input, <code>kpar</code> specifies the kernel parameter (e.g. the inverse kernel width $\\gamma$ in the RBF case or the degree $p$ in the polynomial case.)\n",
    "\t<ol>\n",
    "\t<li>For the linear kernel (<code>ktype='linear'</code>) svm, use $k(\\mathbf{x},\\mathbf{z})=x^Tz$ </li> \n",
    "\t<li>For the radial basis function kernel (<code>ktype='rbf'</code>) svm use $k(\\mathbf{x},\\mathbf{z})=\\exp(-\\gamma ||x-z||^2)$ (gamma is a hyperparameter, passed a the value of kpar)</li>\n",
    "\t<li>For the polynomial kernel (<code>ktype='poly'</code>) use  $k(\\mathbf{x},\\mathbf{z})=(x^Tz + 1)^p$ (p is the degree of the polymial, passed as the value of kpar)</li>\n",
    "</ol>\n",
    "\n",
    "<p>You can use the function <b><code>l2distance</code></b> as a helperfunction.</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "include(\"l2distance.jl\"); # we are pulling this function from the previous assignment\n",
    "#<GRADED>\n",
    "function computeK(kerneltype, X, Z; kpar=0)\n",
    "    \"\"\"\n",
    "    function K = computeK(kernel_type, X, Z)\n",
    "    computes a matrix K such that Kij=g(x,z);\n",
    "    for three different function linear, rbf or polynomial.\n",
    "    \n",
    "    Input:\n",
    "    kerneltype: either 'linear','polynomial','rbf'\n",
    "    X: n input vectors of dimension d (dxn);\n",
    "    Z: m input vectors of dimension d (dxn);\n",
    "    kpar: kernel parameter (inverse kernel width gamma in case of RBF, degree in case of polynomial)\n",
    "    \n",
    "    OUTPUT:\n",
    "    K : nxm kernel matrix\n",
    "    \"\"\"\n",
    "    @assert(kerneltype in [\"linear\",\"polynomial\",\"poly\",\"rbf\"],\"Kernel type not known.\"); # only three kernels defined\n",
    "    @assert(size(X,2)==size(Z,2),\"Input dimensions do not match\");# make sure the dimensions match up\n",
    "\n",
    "    # TODO 2\n",
    "    \n",
    "    return(K); \n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The following code snippet plots an image of the kernel matrix for the data points in the spiral set. Use it to test your <b><code>computeK</code></b> function:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=spiraldata();\n",
    "K=computeK(\"rbf\",xTr,xTr,kpar=0.05);\n",
    "pcolormesh(K); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the SVM optimization has the following dual formulation:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "             &\\min_{\\alpha_1,\\cdots,\\alpha_n}\\frac{1}{2} \\sum_{i,j}\\alpha_i \\alpha_j y_i y_j \\mathbf{K}_{ij} - \\sum_{i=1}^{n}\\alpha_i  \\\\\n",
    "       \\text{s.t.}  &\\quad 0 \\leq \\alpha_i \\leq C\\\\\n",
    "             &\\quad \\sum_{i=1}^{n} \\alpha_i y_i = 0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "This is equivalent to solving for the SVM primal\n",
    "$$ L(\\mathbf{w},b) = C\\sum_{i=1}^n \\max(1-y_i(\\mathbf{w}^\\top\\phi(\\mathbf{x}_i)+b),0) + ||w||_2^2$$\n",
    "where $\\mathbf{w}=\\sum_{i=1}^n y_i \\alpha_i \\phi(\\mathbf{x}_i)$ and $\\mathbf{K}_{ij}=k(\\mathbf{x}_i,\\mathbf{x}_j)=\\phi(\\mathbf{x}_i)^\\top\\phi(\\mathbf{x}_j)$, for some mapping $\\phi(\\cdot)$.  Please note that here all $\\alpha_i\\geq 0$, which is possible because we multiply by $y_i$ in the definition of $\\mathbf{w}$. One advantage of keeping all $\\alpha_i$ non-negative is that we can easily identify non-support vectors as vectors with $\\alpha_i=0$. \n",
    "\n",
    "<p>Implement the function <code>dualqp</code>, which takes as input a kernel matrix $K$, a vector of labels $yTr$ in $\\mathcal{R}^{n}$, and a regularization constant $C\\geq 0$. This function should solve the quadratic optimization problem and output the optimal vector $\\mathbf{\\alpha}\\in{\\mathcal{R}^n}$.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Convex;\n",
    "using SCS;\n",
    "#<GRADED>\n",
    "function dualqp(K,yTr,C);\n",
    "    \"\"\"\n",
    "    function alpha = dualqp(K,yTr,C)\n",
    "    constructs the SVM dual formulation and uses a built-in \n",
    "    convex solver to find the optimal solution. \n",
    "    \n",
    "    Input:\n",
    "        K     | the (nxn) kernel matrix\n",
    "        yTr   | training labels (nx1)\n",
    "        C     | the SVM regularization parameter\n",
    "    \n",
    "    Output:\n",
    "        α     | the calculated solution vector (nx1)\n",
    "    \"\"\"\n",
    "    y=yTr[:];\n",
    "    N=size(K,1);\n",
    "    α=Variable(N);\n",
    "\n",
    "    # TODO 3\n",
    "    \n",
    "    return(α.value[:])\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows a usecase of how <code>dualqp</code> could be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "C=10;\n",
    "λ=0.25;\n",
    "ktype=\"rbf\";\n",
    "# compute kernel (make sure it is PSD)\n",
    "K=computeK(ktype,xTr,xTr,kpar=0.25);\n",
    "ϵ=1e-10;\n",
    "# make sure it is symmetric and positive semi-definite;\n",
    "K=(K+K')./2+ϵ*eye(size(K,1)); \n",
    "\n",
    "α=dualqp(K,yTr,C);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Now that you can solve the dual correctly, you should have the values for $\\alpha_i$. But you are not done yet. You still need to be able to classify new test points. Remember from class that $h(\\mathbf{x})=\\sum_{i=1}^n \\alpha_i y_i k(\\mathbf{x}_i,\\mathbf{x})+b$. You need to obtain $b$. It is easy to show (and omitted here) that if $C>\\alpha_i>0$ (with strict $>$), then we must have that $y_i(\\mathbf{w}^\\top \\phi(\\mathbf{x}_i)+b)=1$. Rephrase this equality in terms of $\\alpha_i$ and solve for $b$. Implement\n",
    "\n",
    "<p> b=recoverBias(K,yTr,alphas,C); </p>\n",
    "\n",
    "<p> where <code>b</code> is the hyperplane bias.\n",
    "(Hint: This is most stable if you pick an $\\alpha_i$ that is furthest from $C$ and $0$. )</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function recoverBias(K,yTr,α,C);\n",
    "    \"\"\"\n",
    "    function bias=recoverBias(K,yTr,α,C);\n",
    "    Solves for the hyperplane bias term, which is uniquely specified by the \n",
    "    support vectors with alpha values 0<alpha<C\n",
    "    \n",
    "    INPUT:\n",
    "    K : nxn kernel matrix\n",
    "    yTr : 1xn input labels\n",
    "    α  : nx1 vector of alpha values\n",
    "    C : regularization constant\n",
    "    \n",
    "    Output:\n",
    "    bias : the scalar hyperplane bias of the kernel SVM specified by alphas\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO 4\n",
    "    \n",
    "    return(bias);\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Test your <b><code>recoverBias</code></b> function with the following code, which uses the dual solver on a linearly separable dataset:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xTr,yTr=genrandomdata(b=0.5);\n",
    "C=10;\n",
    "fun,w,b=primalSVM(xTr,yTr,C=10);\n",
    "K=computeK(\"linear\",xTr,xTr);\n",
    "ϵ=1e-10;\n",
    "K=(K+K')./2+ϵ*eye(size(K,1)); # make sure it is symmetric and positive semi-definite;\n",
    "α=dualqp(K,yTr,C);\n",
    "ba=recoverBias(K,yTr,α,C);\n",
    "wa=(α.*yTr[:])'*xTr;\n",
    "fun=xTe->xTe*wa[:]+ba[1];\n",
    "visclassifier(fun,xTr,yTr,w=wa,b=ba);\n",
    "ba/b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Implement the function \n",
    "    <pre>\n",
    "    svmclassify=dualSVM(xTr,yTr,C,ktype,kpar);\n",
    "    </pre>\n",
    "    It should use your functions <code><b>computeK</b></code> and <code><b>generateQP</b></code> to solve the SVM dual problem of an SVM specified by a training data set (<code><b>xTr,yTr</b></code>), a regularization parameter (<code>C</code>), a kernel type (<code>ktype</code>) and kernel parameter (<code>lmbda</code>, to be used as kpar in Kernel construction). Then, find the support vectors and recover the bias to return <b><code>svmclassify</code></b>, a function that uses your SVM to classify a set of test points <code>xTe</code>.\n",
    "\n",
    "    \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function dualSVM(xTr,yTr,C,ktype,λ);\n",
    "    \"\"\"\n",
    "    function classifier = dualSVM(xTr,yTr,C,ktype,λ);\n",
    "    Constructs the SVM dual formulation and uses a built-in \n",
    "    convex solver to find the optimal solution. \n",
    "    \n",
    "    Input:\n",
    "        xTr   | training data (nxd)\n",
    "        yTr   | training labels (nx1)\n",
    "        C     | the SVM regularization parameter\n",
    "        ktype | the type of kernelization: 'rbf','polynomial','linear'\n",
    "        λ     | the kernel parameter - degree for poly, inverse width for rbf\n",
    "    \n",
    "    Output:\n",
    "        svmclassify | usage: predictions=svmclassify(xTe);\n",
    "    \"\"\"\n",
    "    svmclassify = x->x; #Dummy code\n",
    "    \n",
    "    # TODO 5\n",
    "    \n",
    "    return(svmclassify);\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we try the SVM with RBF kernel on the spiral data. If you implemented it correctly, train and test error should be close to zero.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=spiraldata();\n",
    "C=10.0;\n",
    "σ=0.25;\n",
    "ktype=\"rbf\";\n",
    "svmclassify=dualSVM(xTr,yTr,C,ktype,σ);\n",
    "\n",
    "visclassifier(svmclassify,xTr,yTr);\n",
    "\n",
    "# compute training and testing error\n",
    "predsTr=svmclassify(xTr);\n",
    "trainingerr=mean(sign(predsTr).!=yTr);\n",
    "@printf(\"\\nTraining error: %2.4f\\n\",trainingerr)\n",
    "\n",
    "predsTe=svmclassify(xTe);\n",
    "trainingerr=mean(sign(predsTe).!=yTe);\n",
    "@printf(\"\\Testing error: %2.4f\\n\",trainingerr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are pretty sensitive to hyper-parameters. We can visualize the results of a hyper-parameter grid search as a heat-map, where we sweep across different values of C and kpar and output the result on a validation dataset. Now we ask you to implement a cross-validation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function cross_validation(xTr,yTr,xValid,yValid,ktype,CList,lmbdaList)\n",
    "    \"\"\"\n",
    "    function bestC,bestLmbda,ErrorMatrix = cross_validation(xTr,yTr,xValid,yValid,ktype,CList,lmbdaList);\n",
    "    Use the parameter search to find the optimal parameter,\n",
    "    Individual models are trained on (xTr,yTr) while validated on (xValid,yValid)\n",
    "    \n",
    "    Input:\n",
    "        xTr      | training data (nxd)\n",
    "        yTr      | training labels (nx1)\n",
    "        xValid   | training data (mxd)\n",
    "        yValid   | training labels (mx1)\n",
    "        ktype    | the type of kernelization: 'rbf','polynomial','linear'\n",
    "        CList    | The list of values to try for the SVM regularization parameter C (ax1)\n",
    "        lmbdaList| The list of values to try for the kernel parameter lmbda- degree for poly, inverse width for rbf (bx1)\n",
    "    \n",
    "    Output:\n",
    "        bestC      | the best C parameter\n",
    "        bestLmbda  | the best Lmbda parameter\n",
    "        ErrorMatrix| the test error rate for each given C and Lmbda when trained on (xTr,yTr) and tested on (xValid,yValid),(axb)\n",
    "    \"\"\"\n",
    "    # gridsearch for best parameters\n",
    "    ErrorMatrix=zeros(length(CList),length(lmbdaList));\n",
    "    bestC,bestLmbda = 0.,0.;\n",
    "    \n",
    "    # TODO 6\n",
    "            \n",
    "    return(bestC,bestLmbda,ErrorMatrix)\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gridsearch for best parameters\n",
    "xTr,yTr,xValid,yValid=spiraldata(100);\n",
    "CList=(2.0.^linspace(-1,5,7));\n",
    "lmbdaList=(linspace(0.1,0.5,5));\n",
    "\n",
    "bestC,bestLmbda,ErrorMatrix = cross_validation(xTr,yTr,xValid,yValid,\"rbf\",CList,lmbdaList);\n",
    "\n",
    "pcolormesh(ErrorMatrix);\n",
    "colorbar()\n",
    "xlabel(\"σ\")\n",
    "ylabel(\"C\")\n",
    "title(\"Hyperparameter error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented everything correctly, the result should look similar to this image:\n",
    "<center>\n",
    " <img src=\"crossval.png\" width=\"300px\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competition: we ask you to implement function autosvm, which given xTr and yTr, splits them into training data and validation data, and then uses a hyperparameter search to find the optimal hyper parameters. \n",
    "\n",
    "Function autosvm should return a function which will act as a classifier on xTe.\n",
    "\n",
    "You have a 5 minute time limit on multiple datasets, each dataset having different optimal hyperparameters, so you should strive for a good method of finding hyperparameters (within the time limit) instead of just trying to find a static set of good hyperparameters. \n",
    "\n",
    "You will get extra credit for the competition if you can beat the base benchmark of 34% error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function autosvm(xTr,yTr)\n",
    "\"\"\"\n",
    "svmclassify = autosvm(xTr,yTr), where yTe = svmclassify(xTe)\n",
    "\"\"\"\n",
    "    return x->x[:,1]; # Dummy code\n",
    "    \n",
    "# (Optional) TODO 7\n",
    "\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- -->"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
